{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/pogscafe/invokeai-notebook?scriptVersionId=151255489\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"### Installing InvokeAI - Required","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport subprocess\n\nenv = os.environ.copy()\n\n!pip install 'InvokeAI[xformers]' --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu118 --break-system-packages\n\n!pip install tensorflow\n!pip install --no-deps tensorflow-io \n    \n# Install openssh for running the UI with RemoteMoe\n!mamba install openssh -y\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### \n### Configuration and downloading default models - Required","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/temp/invokeai\n!mkdir /kaggle/temp/invokeai/configs\n\n#@markdown Download only the default model in initial configuration.\n#@markdown Checking this prevents running out of space in Colab.\n\ndefaultOnly = True #@param {type:\"boolean\"}\nskipWeights = True #@param {type:\"boolean\"}\nskipSupportModels = False #@param {type:\"boolean\"}\nnoFullPrecision = True\n\n#@markdown This step usually takes about 2 minutes with only the default model and no weights.\n\n#@markdown You can ignore \"File exists\" warnings in the output.\n\ncmd = 'invokeai-configure --root_dir /kaggle/temp/invokeai --yes'\n\nif defaultOnly:\n  cmd += ' --default_only'\n\nif skipWeights:\n  cmd += ' --skip-sd-weights'\n\nif skipSupportModels:\n  cmd += ' --skip-support-models'\n\nsubprocess.run(cmd, shell=True, env=env)\n\nimport fileinput\nimport os\ndef find(name, path):\n    for root, dirs, files in os.walk(path):\n        if name in files:\n            return os.path.join(root, name)\n\nif noFullPrecision:\n  model_install_file = find('model_install_backend.py', '/opt/conda/lib/')\n  print('modifying file ' + str(model_install_file))\n  for line in fileinput.input(model_install_file, inplace=True):\n    if ('precision = torch_dtype(choose_torch_device())' in line):\n       line = line.replace('torch_dtype(choose_torch_device())', 'torch.float16')\n    print(line, end='')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### \n### Add the SDXL base model (optional)","metadata":{}},{"cell_type":"code","source":"import os.path\nfrom os import path\n\n# Install the SDXL base model\ndef installSdxl(env):\n  installCmd = 'invokeai-model-install --add \"stabilityai/stable-diffusion-xl-base-1.0\" --root_dir /kaggle/temp/invokeai'\n  subprocess.run(installCmd, shell=True, env=env)\n  \nalreadyInstalled = True\nsdxlBaseSubfolderName = ''\nmodelsPath = '/kaggle/working/stablemodels/'\nsdxlBaseSubfolderName = '/stable-diffusion-xl-base-1-0'\nworkSdxlMainFolder = modelsPath + 'sdxl/main'\nif not path.exists(workSdxlMainFolder):\n    os.makedirs(workSdxlMainFolder, exist_ok=True)\n    alreadyInstalled = False\n\ntempModelsSdxlFolder = '/kaggle/temp/invokeai/models/sdxl/'\ntempSdxlMainFolder = tempModelsSdxlFolder + 'main'\n\nsubprocess.run('rm -rf ' + tempModelsSdxlFolder, shell=True, env=env)\nif path.exists(tempModelsSdxlFolder):\n    subprocess.run('rmdir ' + tempModelsSdxlFolder, shell=True, env=env)\n\nif not alreadyInstalled:\n    if not path.exists(tempModelsSdxlFolder):\n      os.makedirs(tempModelsSdxlFolder, exist_ok=True)\n    subprocess.run('ln -s '+workSdxlMainFolder+' '+tempModelsSdxlFolder, shell=True, env=env)\n    installSdxl(env)\nelse:\n    if not path.exists(tempSdxlMainFolder):\n      os.makedirs(tempSdxlMainFolder, exist_ok=True)\n    subprocess.run('ln -s '+workSdxlMainFolder + sdxlBaseSubfolderName+' '+ tempSdxlMainFolder, shell=True, env=env)\n    updateModelsYaml = True\n    with open('/kaggle/temp/invokeai/configs/models.yaml') as f:\n      if 'stable-diffusion-xl-base-1-0' in f.read():\n        updateModelsYaml = False\n    if updateModelsYaml:\n      with open('/kaggle/temp/invokeai/configs/models.yaml', 'a') as file:\n        lines = [\n          'sdxl/main/stable-diffusion-xl-base-1-0:\\n',\n          '  path: sdxl/main/stable-diffusion-xl-base-1-0\\n',\n          '  description: Stable Diffusion XL base model (12 GB)\\n',\n          '  variant: normal\\n',\n          '  format: diffusers\\n'\n        ]\n        file.writelines(lines)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### \n### Add the refiner and vae (optional)","metadata":{}},{"cell_type":"code","source":"#This one took about 14 minutes.\n#Skip this step if you don't need these models.\n!invokeai-model-install --add \"stabilityai/stable-diffusion-xl-refiner-1.0\" --root_dir /kaggle/temp/invokeai --yes\n!invokeai-model-install --add \"madebyollin/sdxl-vae-fp16-fix\" --root_dir /kaggle/temp/invokeai --yes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### \n### Save images to the working folder (optional)\nIf file persistance is enabled, images and the database will be saved for future sessions","metadata":{}},{"cell_type":"code","source":"import os.path\nfrom os import path\n\n# Linking output images to working folder\noutputDrivePath = '/kaggle/working/images/invoke-outputs' #@param {type:\"string\"}\n# Full path to the output folder on Google Drive\n\nsaveDatabase = True #@param {type:\"boolean\"}\n\nif not outputDrivePath.endswith('/'):\n  outputDrivePath = outputDrivePath + '/'\nimagesDrivePath = outputDrivePath + 'images'\ndatabaseDrivePath = outputDrivePath + 'databases'\nif not path.exists(imagesDrivePath):\n  os.makedirs(imagesDrivePath, exist_ok=True)\n\n\noutputsLocalPath = '/kaggle/temp/invokeai/outputs'\nimagesLocalPath = '/kaggle/temp/invokeai/outputs/images'\n\nif not path.exists(outputsLocalPath):\n  os.makedirs(outputsLocalPath, exist_ok=True)\n\nimport datetime\n\nif path.exists(imagesLocalPath):\n    cmd = f'mv {imagesLocalPath} {imagesLocalPath}-backup{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n    subprocess.run(cmd, shell=True, env=env)\n\ncmd = f'ln -s {imagesDrivePath} {outputsLocalPath}'\nsubprocess.run(cmd, shell=True, env=env)\n\n# Linking the database\nif saveDatabase:\n  if not path.exists(databaseDrivePath):\n    os.makedirs(databaseDrivePath, exist_ok=True)\n\n  databaseLocalPath = '/kaggle/temp/invokeai/databases'\n\n  cmd = f'mv {databaseLocalPath} {databaseLocalPath}-backup{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n  subprocess.run(cmd, shell=True, env=env)\n\n  cmd = f'ln -s {databaseDrivePath} /kaggle/temp/invokeai'\n  subprocess.run(cmd, shell=True, env=env)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### \n### Start the WebUI","metadata":{}},{"cell_type":"markdown","source":"**Option 1: Starting the Web UI with ngrok**  \n* Make sure to put your ngrok token in the Ngrok_token variable. The token can be obtained from https://ngrok.com\n* If you have a static domain, put your ngrok domain in the Ngrok_domain variable.\n* Wait for the line that says \"Uvicorn running on http://127.0.0.1:9090 (Press CTRL+C to quit)\" \n* Visit your ngrok URL (either your static domain, or the ngrok url displayed in the output)","metadata":{}},{"cell_type":"code","source":"# Option 1: Starting the Web UI with ngrok\n\nNgrok_token = \"\" #@param {type:\"string\"}\n# Put your ngrok token here (obtainable from https://ngrok.com)\n\nNgrok_domain = \"\" # optional, leave empty if you don't have a domain\n\n# Only works with InvokeAI 3.0.2 and later\n\n!pip install pyngrok\n\nfrom pyngrok import ngrok, conf\nimport fileinput\nimport sys\n\nif Ngrok_token!=\"\":\n  ngrok.kill()\n  srv=ngrok.connect(9090 , pyngrok_config=conf.PyngrokConfig(auth_token=Ngrok_token),\n                    bind_tls=True, domain=Ngrok_domain).public_url\n  print(srv)\n  get_ipython().system(\"invokeai-web  --root /kaggle/temp/invokeai/\")\nelse:\n  print('An ngrok token is required. You can get one on https://ngrok.com and paste it into the ngrok_token field.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***   \n**Option 2: Starting the Web UI with RemoteMoe**  \n* Wait for the line that says \"Uvicorn running on http://127.0.0.1:9090 (Press CTRL+C to quit)\"   \n* Click the link that ends with .remote.moe","metadata":{}},{"cell_type":"code","source":"#Option 2: Starting the Web UI with RemoteMoe\n\n# Wait for the line that says \"Uvicorn running on http://127.0.0.1:9090 (Press CTRL+C to quit)\" \n# and click on the link that ends with .remote.moe\n\n!mkdir  ~/.ssh/\n!touch  ~/.ssh/known_hosts\n!ssh-keyscan -t rsa remote.moe >> ~/.ssh/known_hosts\n!rm /root/.ssh/id_rsa\n!ssh-keygen -t rsa -b 4096 -f /root/.ssh/id_rsa -q -N \"\"\n#!ssh -R 80:127.0.0.1:9090 -o StrictHostKeyChecking=no -i /root/.ssh/id_rsa remote.moe \n!invokeai-web --root /kaggle/temp/invokeai/ & ssh -R 80:127.0.0.1:9090 -o StrictHostKeyChecking=no -i /root/.ssh/id_rsa remote.moe ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***   \n**Option 3: Starting the Web UI with Localtunnel**  \n* Copy the IP address shown in the output above the line \"your url is: https://some-random-words.loca.lt\"  \n* Wait for the line that says \"Uvicorn running on http://127.0.0.1:9090 (Press CTRL+C to quit)\"   \n* Click the localtunnel url and paste the IP you copied earlier to the \"Endpoint IP\" text field\n","metadata":{}},{"cell_type":"code","source":"# Option 3: Starting the Web UI with Localtunnel\n# Warning: Localtunnel has been down recently. If the .loca.lt link doesn't show up in the output\n# please use one of the other two options\n\n%cd /kaggle/temp/invokeai/\n!npm install -g localtunnel\n\n#@markdown Copy the IP address shown in the output above the line\n#@markdown \"your url is: https://some-random-words.loca.lt\"\n!wget -q -O - ipv4.icanhazip.com\n\n#@markdown Wait for the line that says \"Uvicorn running on http://127.0.0.1:9090 (Press CTRL+C to quit)\"\n\n#@markdown Click the localtunnel url and paste the IP you copied earlier to the \"Endpoint IP\" text field\n!lt --port 9090 --local_https False & invokeai-web  --root /kaggle/temp/invokeai/ --ignore_missing_core_models \n\n#@markdown If the UI shows a red dot that says 'disconnected' when hovered in the upper\n#@markdown right corner and the Invoke button is disabled, change 'https' to 'http'\n#@markdown in the browser's address bar and press enter.\n#@markdown When the page reloads, the UI should work properly.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}